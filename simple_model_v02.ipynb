{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import missingno as msno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 предобработка данных \n",
    "- проверить наличие пустых значений\n",
    "- понять чем заполнить(среднее, нули)\n",
    "- преобразовать стринговые переменные в dummy при необходимости\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 построение моделей\n",
    "- оптимизируемся на разные метрики\n",
    "- разные типы кросс-валидации\n",
    "- разная предобработка категориальных признаков \n",
    "- разный тип перебора гиперпараметров\n",
    "- отбор признаков\n",
    "- KNN, лин. модели, лин с регуляризацией, ансамбли (случайный лес, 3 вида бустингов(от 3 разных компаний))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 интерпретация результатов\n",
    "- выбор лучшей модели исходя из того как они ведут себя на тестовой выборке (precision, recall, f1 , ROC_AUC, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_qual_line(model,data):\n",
    "    model_name = model.best_params_\n",
    "    model_accuracy = accuracy_score(y_holdout, model.predict(X_holdout))\n",
    "    model_recall = recall_score(y_holdout, model.predict(X_holdout))\n",
    "\n",
    "    predict_qual = {'accuracy': model_accuracy,\n",
    "                    'recall': model_recall,\n",
    "                    'model': model_name}\n",
    "    data = data.append(predict_qual, ignore_index=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('https://raw.githubusercontent.com/rebeccabilbro/titanic/master/data/train.csv')\n",
    "#выбросим лишнее\n",
    "data_raw.drop(['Cabin','Ticket','Name', 'PassengerId' ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0           0       3    male  22.0      1      0   7.2500        S\n",
       "1           1       1  female  38.0      1      0  71.2833        C\n",
       "2           1       3  female  26.0      0      0   7.9250        S\n",
       "3           1       1  female  35.0      1      0  53.1000        S\n",
       "4           0       3    male  35.0      0      0   8.0500        S\n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...\n",
       "886         0       2    male  27.0      0      0  13.0000        S\n",
       "887         1       1  female  19.0      0      0  30.0000        S\n",
       "888         0       3  female   NaN      1      2  23.4500        S\n",
       "889         1       1    male  26.0      0      0  30.0000        C\n",
       "890         0       3    male  32.0      0      0   7.7500        Q\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survived': 0,\n",
       " 'Pclass': 0,\n",
       " 'Sex': 0,\n",
       " 'Age': 177,\n",
       " 'SibSp': 0,\n",
       " 'Parch': 0,\n",
       " 'Fare': 0,\n",
       " 'Embarked': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#проверим пропуски\n",
    "{key:data_raw[key].isna().sum() for key in data_raw.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "msno.bar(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "msno.matrix(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw.copy()\n",
    "\n",
    "\n",
    "median_age = data['Age'].median()\n",
    "data['Age'].fillna(median_age, inplace=True) \n",
    "data['Embarked'].fillna(data['Embarked'].value_counts().index[0], inplace=True) #заменим пропуски последующим значением \n",
    "\n",
    "y = data['Survived']\n",
    "X = data.drop(['Survived'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим 70% выборки (X_train, y_train) под обучение и 30% в качестве отложенной выборки (X_holdout, y_holdout). Отложенная выборка никак не будет участвовать в настройке параметров моделей, на ней мы в конце, после этой настройки, оценим качество полученной модели.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3,\n",
    "                                                          random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding для пола\n",
    "feature_categorical = [\"Sex\",\"Embarked\", 'Pclass']\n",
    "feature_categoricals = ['Parch','SibSp']#категориальные, с более чем 2 категориями\n",
    "X_train_prep = pd.get_dummies(X_train, columns=feature_categorical, drop_first=True)\n",
    "X_holdout_prep = pd.get_dummies(X_holdout, columns=feature_categorical, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15.7417</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.7500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15.5000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>623 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  SibSp  Parch      Fare  Sex_male  Embarked_Q  Embarked_S  Pclass_2  \\\n",
       "66   29.0      0      0   10.5000         0           0           1         1   \n",
       "381   1.0      0      2   15.7417         0           0           0         0   \n",
       "223  28.0      0      0    7.8958         1           0           1         0   \n",
       "629  28.0      0      0    7.7333         1           1           0         0   \n",
       "337  41.0      0      0  134.5000         0           0           0         0   \n",
       "..    ...    ...    ...       ...       ...         ...         ...       ...   \n",
       "406  51.0      0      0    7.7500         1           0           1         0   \n",
       "390  36.0      1      2  120.0000         1           0           1         0   \n",
       "143  19.0      0      0    6.7500         1           1           0         0   \n",
       "241  28.0      1      0   15.5000         0           1           0         0   \n",
       "623  21.0      0      0    7.8542         1           0           1         0   \n",
       "\n",
       "     Pclass_3  \n",
       "66          0  \n",
       "381         1  \n",
       "223         1  \n",
       "629         1  \n",
       "337         0  \n",
       "..        ...  \n",
       "406         1  \n",
       "390         0  \n",
       "143         1  \n",
       "241         1  \n",
       "623         1  \n",
       "\n",
       "[623 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.metrics import (roc_auc_score, recall_score, f1_score, precision_score,\n",
    "                            accuracy_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from catboost import Pool, cv\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers as L         # Уже готовые слои для моделей\n",
    "from tensorflow.keras.models import Sequential   # Специальный класс для склеивания слоёв\n",
    "from tensorflow.keras.models import Model        # Альтернативный класс для склейки слоёв\n",
    "import tensorflow.keras.optimizers as opt        # Разные оптимизационные алгоритмы :3 \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#преобразование переменных\n",
    "\n",
    "categorical_features = [\"Sex\",\"Embarked\", 'Pclass',\n",
    "                        'Parch','SibSp',\n",
    "                        ]\n",
    "# categorical_features = [\"installs_interval\", 'genre_id', 'country']\n",
    "for col_cat in categorical_features:\n",
    "    X_train[col_cat] = X_train[col_cat].astype(\"category\")\n",
    "    X_holdout[col_cat] = X_holdout[col_cat].astype(\"category\")\n",
    "\n",
    "numeric_features = [i for i in X_train.columns if i not in categorical_features]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(with_std=True, with_mean=True))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', ce.OneHotEncoder(use_cat_names=True))])\n",
    "\n",
    "categorical_transformer_catboost = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))])\n",
    "\n",
    "# categorical_transformer = OneHotEncoder(drop='if_binary')\n",
    "# categorical_transformer = DataFrameOneHotEncoder(col_overrule_params={\"in_app_purchase\":{\"drop\":\"first\"}})\n",
    "# categorical_transformer = ce.GLMMEncoder()\n",
    "# categorical_transformer = ce.CatBoostEncoder()\n",
    "\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "# Catboost preprocessor\n",
    "preprocessor_catboost = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer_catboost, categorical_features)])\n",
    "\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_holdout_prep = preprocessor.transform(X_holdout)\n",
    "\n",
    "#выбор модели\n",
    "\n",
    "# Logistic regression\n",
    "\n",
    "pipe_logistic = Pipeline([('scl', preprocessor),\n",
    "                        ('clf', LogisticRegression(penalty=\"l2\",\n",
    "                                                   solver='liblinear'))\n",
    "                        ])  # pipeline with all steps\n",
    "param_dict_logistic = {'clf__C': np.linspace(0.01, 10, 1000)\n",
    "                       }\n",
    "\n",
    "#кросс-валидация, подбор гипер-параметров\n",
    "skf = StratifiedKFold(n_splits=5)  # CV type\n",
    "logistic_randomized_pipe = RandomizedSearchCV(estimator=pipe_logistic,\n",
    "                                              param_distributions=param_dict_logistic,\n",
    "                                              cv=skf, n_iter=30, n_jobs=-1)\n",
    "\n",
    "#Random Forest\n",
    "\n",
    "pipe_rndforest = Pipeline([('scl', preprocessor),\n",
    "                           ('clf', RandomForestClassifier( random_state=13))\n",
    "                        ])  # pipeline with all steps\n",
    "param_dict_rndforest = {'clf__max_depth': np.arange(1,10),\n",
    "                        'clf__min_samples_leaf': np.arange(1, 10),\n",
    "                        'clf__n_estimators': [100, 200, 300]\n",
    "                       }\n",
    "\n",
    "#кросс-валидация, подбор гипер-параметров\n",
    "# skf = StratifiedKFold(n_splits=5)  # CV type\n",
    "rndforest_randomized_pipe = RandomizedSearchCV(estimator=pipe_rndforest,\n",
    "                                              param_distributions=param_dict_rndforest,\n",
    "                                              cv=skf, n_iter=30, n_jobs=-1)\n",
    "\n",
    "\n",
    "#KNN\n",
    "\n",
    "pipe_knn = Pipeline([('scl', preprocessor),\n",
    "                        ('clf', KNeighborsClassifier())\n",
    "                        ])  # pipeline with all steps\n",
    "param_dist_knn = {'clf__n_neighbors': np.arange(1,20),\n",
    "                   'clf__p': np.arange(1, 5)\n",
    "                 }\n",
    "\n",
    "# #кросс-валидация, подбор гипер-параметров\n",
    "# skf = StratifiedKFold(n_splits=5)  # CV type\n",
    "knn_randomized_pipe = GridSearchCV(estimator=pipe_knn,\n",
    "                                              param_grid=param_dist_knn,\n",
    "                                              cv=skf, n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "#CatBoost\n",
    "\n",
    "# pipe_catboost = Pipeline([('scl', preprocessor_catboost),\n",
    "#                            ('clf', CatBoostClassifier(random_state=13, cat_features=categorical_features))\n",
    "pipe_catboost = Pipeline([('scl', preprocessor),\n",
    "                           ('clf', CatBoostClassifier(random_state=13))\n",
    "                        ])  # pipeline with all steps\n",
    "param_dict_catboost = {'max_depth': np.arange(1,10),\n",
    "                        'n_estimators': [100, 200, 300],\n",
    "                        'learning_rate': np.linspace(0.01, 0.3, 10),\n",
    "                        'l2_leaf_reg': np.linspace(0.01, 0.5, 10),\n",
    "                       'min_data_in_leaf': np.arange(1, 10)\n",
    "                       }\n",
    "param_dict_catboost = {\"clf__\" + key: value for key, value in param_dict_catboost.items()}\n",
    "\n",
    "catboost_randomized_pipe = RandomizedSearchCV(estimator=pipe_catboost,\n",
    "                                              param_distributions=param_dict_catboost,\n",
    "                                              cv=skf, n_iter=30, n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#XGB\n",
    "\n",
    "# pipe_catboost = Pipeline([('scl', preprocessor_catboost),\n",
    "#                            ('clf', CatBoostClassifier(random_state=13, cat_features=categorical_features))\n",
    "pipe_xgb = Pipeline([('scl', preprocessor),\n",
    "                           ('clf', XGBClassifier(random_state=13))\n",
    "                        ])  # pipeline with all steps\n",
    "param_dict_xgb = {'max_depth': np.arange(1,10),\n",
    "                        'n_estimators': [100, 200, 300],\n",
    "                        'learning_rate': np.linspace(0.01, 0.3, 10),\n",
    "                        'l2_leaf_reg': np.linspace(0.01, 0.5, 10),\n",
    "                       'min_data_in_leaf': np.arange(1, 10)\n",
    "                       }\n",
    "param_dict_xgb = {\"clf__\" + key: value for key, value in param_dict_xgb.items()}\n",
    "\n",
    "xgb_randomized_pipe = RandomizedSearchCV(estimator=pipe_xgb,\n",
    "                                              param_distributions=param_dict_xgb,\n",
    "                                              cv=skf, n_iter=30, n_jobs=-1)\n",
    "\n",
    "\n",
    "#Neural\n",
    "\n",
    "\n",
    "def get_new_model( ):\n",
    "\n",
    "    \n",
    "    ###########################################################\n",
    "    # Ваш код! \n",
    "    model = Sequential(name = 'Archibald')  # модели можно дать имя!\n",
    "    \n",
    "    # Добавляем в нашу модель первый слой из 25 нейронов\n",
    "    model.add(L.Dense(25, input_dim = X_train_prep.shape[1], kernel_initializer='random_normal'))\n",
    "\n",
    "    # Добавляем функцию активации на первый слой \n",
    "    model.add(L.Activation('sigmoid'))\n",
    "\n",
    "    # Добавляем ещё один слой из 25 нейронов\n",
    "    model.add(L.Dense(25, kernel_initializer='random_normal'))\n",
    "    model.add(L.Activation('sigmoid'))\n",
    "\n",
    "    # На выходе мы должны получить вероятности того, что объект относится к разным классам \n",
    "    # Сделать такое преобразование позволяет softmax как функция активации\n",
    "    # На выход будет идти 4 вероятности по числу классов\n",
    "    model.add(L.Dense(2, activation='softmax', kernel_initializer = 'random_normal'))\n",
    "    \n",
    "    ###########################################################\n",
    "    \n",
    "    # В качестве оптимизации будем использовать Adam\n",
    "    # Это такой специальный градиентный спуск, обсудим его в следущий раз\n",
    "    optimizer = opt.Adam(lr=1e-3)\n",
    "\n",
    "    # Собираем модель\n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "                  metrics=[\"accuracy\"], \n",
    "                  optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "clf_neuron = KerasClassifier(build_fn=get_new_model, \n",
    "#                              **{'verbose':0, \n",
    "#                                                         'validation_split':0.2, \n",
    "#                                                         'epochs':300, \n",
    "#                                                         'verbose':1}\n",
    "                            )\n",
    "\n",
    "\n",
    "param_dict_neural = {'batch_size':[10, 20, 40, 60, 80, 100],\n",
    "                  'epochs':[10, 50, 100]}\n",
    "\n",
    "\n",
    "pipe_neuron = Pipeline([('scl', preprocessor),\n",
    "                           ('clf', clf_neuron)\n",
    "                        ])  # pipeline with all steps\n",
    "\n",
    "# grid = GridSearchCV(estimator=pipe_neural, param_grid=param_dict_neural, n_jobs=-1, cv=3)\n",
    "                       \n",
    "param_dict_neural = {\"clf__\" + key: value for key, value in param_dict_neural.items()}\n",
    "\n",
    "neuron_randomized_pipe = GridSearchCV(estimator=pipe_neuron,\n",
    "                                              param_grid=param_dict_neural,\n",
    "                                              cv=skf, n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 0s 512us/step - loss: 0.6680 - accuracy: 0.6035\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 464us/step - loss: 0.6576 - accuracy: 0.6292\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 480us/step - loss: 0.6549 - accuracy: 0.6292\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 480us/step - loss: 0.6484 - accuracy: 0.6292\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 736us/step - loss: 0.6369 - accuracy: 0.6292\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 704us/step - loss: 0.6212 - accuracy: 0.6292\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 616us/step - loss: 0.5971 - accuracy: 0.6437\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 744us/step - loss: 0.5679 - accuracy: 0.7111\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 448us/step - loss: 0.5357 - accuracy: 0.7640\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 456us/step - loss: 0.5100 - accuracy: 0.7817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=Pipeline(steps=[('scl',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         Pipeline(steps=[('imputer',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('scaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['Age',\n",
       "                                                                          'Fare']),\n",
       "                                                                        ('cat',\n",
       "                                                                         Pipeline(steps=[('imputer',\n",
       "                                                                                          SimpleImputer(strategy='most_frequent')),\n",
       "                                                                                         ('ohe',\n",
       "                                                                                          OneHotEncoder(use_cat_names=True))]),\n",
       "                                                                         ['Sex',\n",
       "                                                                          'Embarked',\n",
       "                                                                          'Pclass',\n",
       "                                                                          'Parch',\n",
       "                                                                          'SibSp'])])),\n",
       "                                       ('clf',\n",
       "                                        <keras.wrappers.scikit_learn.KerasClassifier object at 0x00000248D1C05B80>)]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__batch_size': [10, 20, 40, 60, 80, 100],\n",
       "                         'clf__epochs': [10, 50, 100]})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_randomized_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_randomized_pipe.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model( ):\n",
    "\n",
    "    \n",
    "    ###########################################################\n",
    "    # Ваш код! \n",
    "    model = Sequential(name = 'Archibald')  # модели можно дать имя!\n",
    "    \n",
    "    # Добавляем в нашу модель первый слой из 25 нейронов\n",
    "    model.add(L.Dense(25, input_dim = X_train_prep.shape[1], kernel_initializer='random_normal'))\n",
    "\n",
    "    # Добавляем функцию активации на первый слой \n",
    "    model.add(L.Activation('sigmoid'))\n",
    "\n",
    "    # Добавляем ещё один слой из 25 нейронов\n",
    "    model.add(L.Dense(25, kernel_initializer='random_normal'))\n",
    "    model.add(L.Activation('sigmoid'))\n",
    "\n",
    "    # На выходе мы должны получить вероятности того, что объект относится к разным классам \n",
    "    # Сделать такое преобразование позволяет softmax как функция активации\n",
    "    # На выход будет идти 4 вероятности по числу классов\n",
    "    model.add(L.Dense(2, activation='softmax', kernel_initializer = 'random_normal'))\n",
    "    \n",
    "    ###########################################################\n",
    "    \n",
    "    # В качестве оптимизации будем использовать Adam\n",
    "    # Это такой специальный градиентный спуск, обсудим его в следущий раз\n",
    "    optimizer = opt.Adam(lr=1e-3)\n",
    "\n",
    "    # Собираем модель\n",
    "    model.compile(loss = 'categorical_crossentropy', \n",
    "                  metrics=[\"accuracy\"], \n",
    "                  optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6013891\ttotal: 143ms\tremaining: 14.1s\n",
      "1:\tlearn: 0.5557445\ttotal: 143ms\tremaining: 7.03s\n",
      "2:\tlearn: 0.4978956\ttotal: 144ms\tremaining: 4.66s\n",
      "3:\tlearn: 0.4765148\ttotal: 145ms\tremaining: 3.47s\n",
      "4:\tlearn: 0.4423283\ttotal: 145ms\tremaining: 2.76s\n",
      "5:\tlearn: 0.4352946\ttotal: 146ms\tremaining: 2.29s\n",
      "6:\tlearn: 0.4285275\ttotal: 147ms\tremaining: 1.95s\n",
      "7:\tlearn: 0.4175913\ttotal: 147ms\tremaining: 1.69s\n",
      "8:\tlearn: 0.4126963\ttotal: 148ms\tremaining: 1.49s\n",
      "9:\tlearn: 0.4085849\ttotal: 148ms\tremaining: 1.34s\n",
      "10:\tlearn: 0.4047534\ttotal: 149ms\tremaining: 1.21s\n",
      "11:\tlearn: 0.3930161\ttotal: 150ms\tremaining: 1.1s\n",
      "12:\tlearn: 0.3864055\ttotal: 150ms\tremaining: 1.01s\n",
      "13:\tlearn: 0.3806248\ttotal: 151ms\tremaining: 928ms\n",
      "14:\tlearn: 0.3780394\ttotal: 152ms\tremaining: 860ms\n",
      "15:\tlearn: 0.3757362\ttotal: 153ms\tremaining: 801ms\n",
      "16:\tlearn: 0.3746722\ttotal: 153ms\tremaining: 747ms\n",
      "17:\tlearn: 0.3715573\ttotal: 154ms\tremaining: 700ms\n",
      "18:\tlearn: 0.3693237\ttotal: 154ms\tremaining: 657ms\n",
      "19:\tlearn: 0.3675427\ttotal: 155ms\tremaining: 619ms\n",
      "20:\tlearn: 0.3656076\ttotal: 155ms\tremaining: 584ms\n",
      "21:\tlearn: 0.3626336\ttotal: 156ms\tremaining: 552ms\n",
      "22:\tlearn: 0.3608433\ttotal: 156ms\tremaining: 523ms\n",
      "23:\tlearn: 0.3601098\ttotal: 157ms\tremaining: 497ms\n",
      "24:\tlearn: 0.3577950\ttotal: 157ms\tremaining: 472ms\n",
      "25:\tlearn: 0.3555435\ttotal: 158ms\tremaining: 449ms\n",
      "26:\tlearn: 0.3544526\ttotal: 158ms\tremaining: 428ms\n",
      "27:\tlearn: 0.3526394\ttotal: 159ms\tremaining: 408ms\n",
      "28:\tlearn: 0.3477883\ttotal: 159ms\tremaining: 390ms\n",
      "29:\tlearn: 0.3454557\ttotal: 160ms\tremaining: 373ms\n",
      "30:\tlearn: 0.3423117\ttotal: 160ms\tremaining: 357ms\n",
      "31:\tlearn: 0.3413750\ttotal: 161ms\tremaining: 342ms\n",
      "32:\tlearn: 0.3388536\ttotal: 161ms\tremaining: 328ms\n",
      "33:\tlearn: 0.3344441\ttotal: 162ms\tremaining: 314ms\n",
      "34:\tlearn: 0.3297772\ttotal: 162ms\tremaining: 302ms\n",
      "35:\tlearn: 0.3287036\ttotal: 163ms\tremaining: 290ms\n",
      "36:\tlearn: 0.3264610\ttotal: 163ms\tremaining: 278ms\n",
      "37:\tlearn: 0.3243809\ttotal: 164ms\tremaining: 268ms\n",
      "38:\tlearn: 0.3195600\ttotal: 164ms\tremaining: 257ms\n",
      "39:\tlearn: 0.3186204\ttotal: 165ms\tremaining: 247ms\n",
      "40:\tlearn: 0.3178973\ttotal: 165ms\tremaining: 238ms\n",
      "41:\tlearn: 0.3155107\ttotal: 166ms\tremaining: 229ms\n",
      "42:\tlearn: 0.3137680\ttotal: 166ms\tremaining: 221ms\n",
      "43:\tlearn: 0.3119291\ttotal: 167ms\tremaining: 212ms\n",
      "44:\tlearn: 0.3091865\ttotal: 167ms\tremaining: 205ms\n",
      "45:\tlearn: 0.3080696\ttotal: 168ms\tremaining: 197ms\n",
      "46:\tlearn: 0.3063996\ttotal: 168ms\tremaining: 190ms\n",
      "47:\tlearn: 0.3046205\ttotal: 169ms\tremaining: 183ms\n",
      "48:\tlearn: 0.3035015\ttotal: 169ms\tremaining: 176ms\n",
      "49:\tlearn: 0.3006156\ttotal: 170ms\tremaining: 170ms\n",
      "50:\tlearn: 0.2981543\ttotal: 170ms\tremaining: 164ms\n",
      "51:\tlearn: 0.2967345\ttotal: 171ms\tremaining: 158ms\n",
      "52:\tlearn: 0.2930525\ttotal: 171ms\tremaining: 152ms\n",
      "53:\tlearn: 0.2919339\ttotal: 172ms\tremaining: 146ms\n",
      "54:\tlearn: 0.2901485\ttotal: 172ms\tremaining: 141ms\n",
      "55:\tlearn: 0.2888820\ttotal: 173ms\tremaining: 136ms\n",
      "56:\tlearn: 0.2883640\ttotal: 173ms\tremaining: 131ms\n",
      "57:\tlearn: 0.2856933\ttotal: 174ms\tremaining: 126ms\n",
      "58:\tlearn: 0.2837565\ttotal: 174ms\tremaining: 121ms\n",
      "59:\tlearn: 0.2816763\ttotal: 175ms\tremaining: 117ms\n",
      "60:\tlearn: 0.2800531\ttotal: 175ms\tremaining: 112ms\n",
      "61:\tlearn: 0.2777911\ttotal: 176ms\tremaining: 108ms\n",
      "62:\tlearn: 0.2764568\ttotal: 176ms\tremaining: 104ms\n",
      "63:\tlearn: 0.2740070\ttotal: 177ms\tremaining: 99.5ms\n",
      "64:\tlearn: 0.2713360\ttotal: 177ms\tremaining: 95.5ms\n",
      "65:\tlearn: 0.2692082\ttotal: 178ms\tremaining: 91.6ms\n",
      "66:\tlearn: 0.2677774\ttotal: 178ms\tremaining: 87.9ms\n",
      "67:\tlearn: 0.2651383\ttotal: 179ms\tremaining: 84.2ms\n",
      "68:\tlearn: 0.2634692\ttotal: 179ms\tremaining: 80.6ms\n",
      "69:\tlearn: 0.2618283\ttotal: 180ms\tremaining: 77.1ms\n",
      "70:\tlearn: 0.2584062\ttotal: 180ms\tremaining: 73.7ms\n",
      "71:\tlearn: 0.2569890\ttotal: 181ms\tremaining: 70.4ms\n",
      "72:\tlearn: 0.2547114\ttotal: 181ms\tremaining: 67.1ms\n",
      "73:\tlearn: 0.2502269\ttotal: 182ms\tremaining: 64ms\n",
      "74:\tlearn: 0.2475975\ttotal: 183ms\tremaining: 60.9ms\n",
      "75:\tlearn: 0.2437017\ttotal: 183ms\tremaining: 57.9ms\n",
      "76:\tlearn: 0.2406186\ttotal: 184ms\tremaining: 54.9ms\n",
      "77:\tlearn: 0.2379717\ttotal: 184ms\tremaining: 52ms\n",
      "78:\tlearn: 0.2370549\ttotal: 185ms\tremaining: 49.1ms\n",
      "79:\tlearn: 0.2348929\ttotal: 185ms\tremaining: 46.3ms\n",
      "80:\tlearn: 0.2345415\ttotal: 186ms\tremaining: 43.6ms\n",
      "81:\tlearn: 0.2324304\ttotal: 186ms\tremaining: 40.9ms\n",
      "82:\tlearn: 0.2306987\ttotal: 187ms\tremaining: 38.3ms\n",
      "83:\tlearn: 0.2300590\ttotal: 187ms\tremaining: 35.7ms\n",
      "84:\tlearn: 0.2287925\ttotal: 188ms\tremaining: 33.1ms\n",
      "85:\tlearn: 0.2275082\ttotal: 188ms\tremaining: 30.6ms\n",
      "86:\tlearn: 0.2268191\ttotal: 189ms\tremaining: 28.2ms\n",
      "87:\tlearn: 0.2256764\ttotal: 189ms\tremaining: 25.8ms\n",
      "88:\tlearn: 0.2242031\ttotal: 190ms\tremaining: 23.4ms\n",
      "89:\tlearn: 0.2233796\ttotal: 190ms\tremaining: 21.1ms\n",
      "90:\tlearn: 0.2223965\ttotal: 191ms\tremaining: 18.9ms\n",
      "91:\tlearn: 0.2215774\ttotal: 191ms\tremaining: 16.6ms\n",
      "92:\tlearn: 0.2201273\ttotal: 192ms\tremaining: 14.4ms\n",
      "93:\tlearn: 0.2191683\ttotal: 192ms\tremaining: 12.3ms\n",
      "94:\tlearn: 0.2183947\ttotal: 193ms\tremaining: 10.1ms\n",
      "95:\tlearn: 0.2175681\ttotal: 193ms\tremaining: 8.05ms\n",
      "96:\tlearn: 0.2168468\ttotal: 194ms\tremaining: 5.99ms\n",
      "97:\tlearn: 0.2149164\ttotal: 194ms\tremaining: 3.97ms\n",
      "98:\tlearn: 0.2137389\ttotal: 195ms\tremaining: 1.97ms\n",
      "99:\tlearn: 0.2097152\ttotal: 195ms\tremaining: 0us\n",
      "[22:51:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:576: \n",
      "Parameters: { \"l2_leaf_reg\", \"min_data_in_leaf\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:51:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "20/20 [==============================] - 0s 548us/step - loss: 0.6630 - accuracy: 0.6292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scl',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Age', 'Fare']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(use_cat_names=True))]),\n",
       "                                                  ['Sex', 'Embarked', 'Pclass',\n",
       "                                                   'Parch', 'SibSp'])])),\n",
       "                ('clf',\n",
       "                 <keras.wrappers.scikit_learn.KerasClassifier object at 0x00000248D1C05B80>)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_randomized_pipe.fit(X_train,y_train)\n",
    "rndforest_randomized_pipe.fit(X_train,y_train)\n",
    "knn_randomized_pipe.fit(X_train,y_train)\n",
    "catboost_randomized_pipe.fit(X_train ,y_train)\n",
    "xgb_randomized_pipe.fit(X_train ,y_train)\n",
    "pipe_neuron.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model = logistic_randomized_pipe.best_estimator_\n",
    "\n",
    "# Extract type model\n",
    "model_type  = type(best_model.named_steps['clf'])\n",
    "\n",
    "preprocessing_steps_model = best_model.named_steps['scl'].transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выводим результат\n",
    "\n",
    "models_quality = pd.DataFrame( columns = ['Name', 'accuracy_score','recall_score','f1_score',\n",
    "                                          'precision_score','roc_auc_score'])\n",
    "\n",
    "\n",
    "models_names = [logistic_randomized_pipe, rndforest_randomized_pipe, knn_randomized_pipe, \n",
    "                catboost_randomized_pipe, xgb_randomized_pipe,\n",
    "               pipe_neuron\n",
    "               ]\n",
    "models_string = ['logistic_randomized_pipe', 'rndforest_randomized_pipe', 'knn_randomized_pipe',\n",
    "                 'catboost_randomized_pipe',\n",
    "                 'xgb_randomized_pipe',\n",
    "                'pipe_neuron'\n",
    "                ]\n",
    "\n",
    "\n",
    "for i in range(len(models_names)):\n",
    "    model = models_names[i]\n",
    "    log_pred = np.round(model.predict(X_holdout), 0)\n",
    "    log_pred_proba = model.predict_proba(X_holdout)[:,1]\n",
    "    models_quality = models_quality.append({'Name':models_string[i], \n",
    "                                                'accuracy_score':accuracy_score(y_holdout, log_pred),\n",
    "                                                'recall_score':recall_score(y_holdout, log_pred),\n",
    "                                               'f1_score':f1_score(y_holdout, log_pred),\n",
    "                                                'precision_score':precision_score(y_holdout, log_pred),\n",
    "                                               'roc_auc_score':roc_auc_score(y_holdout, log_pred_proba)}, ignore_index=True)\n",
    "#     except ValueError:\n",
    "#         prediction_binary = list(map(round, log_pred))\n",
    "#         models_quality = models_quality.append({'Name':models_string[i],\n",
    "#                                                 'accuracy_score':accuracy_score(y_holdout, prediction_binary),\n",
    "#                                                 'recall_score':recall_score(y_holdout, prediction_binary),\n",
    "#                                                 'f1_score':f1_score(y_holdout, prediction_binary),\n",
    "#                                                 'precision_score':precision_score(y_holdout, prediction_binary),\n",
    "#                                                'roc_auc_score':roc_auc_score(y_holdout, prediction_binary)}, ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>accuracy_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_randomized_pipe</td>\n",
       "      <td>0.768657</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.839932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rndforest_randomized_pipe</td>\n",
       "      <td>0.783582</td>\n",
       "      <td>0.585586</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.846244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>knn_randomized_pipe</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.576577</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.813393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>catboost_randomized_pipe</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.711712</td>\n",
       "      <td>0.745283</td>\n",
       "      <td>0.782178</td>\n",
       "      <td>0.848884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgb_randomized_pipe</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>0.792079</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pipe_neuron</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0.585586</td>\n",
       "      <td>0.673575</td>\n",
       "      <td>0.792683</td>\n",
       "      <td>0.832645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name  accuracy_score  recall_score  f1_score  \\\n",
       "0   logistic_randomized_pipe        0.768657      0.666667  0.704762   \n",
       "1  rndforest_randomized_pipe        0.783582      0.585586  0.691489   \n",
       "2        knn_randomized_pipe        0.746269      0.576577  0.653061   \n",
       "3   catboost_randomized_pipe        0.798507      0.711712  0.745283   \n",
       "4        xgb_randomized_pipe        0.805970      0.720721  0.754717   \n",
       "5                pipe_neuron        0.764925      0.585586  0.673575   \n",
       "\n",
       "   precision_score  roc_auc_score  \n",
       "0         0.747475       0.839932  \n",
       "1         0.844156       0.846244  \n",
       "2         0.752941       0.813393  \n",
       "3         0.782178       0.848884  \n",
       "4         0.792079       0.833333  \n",
       "5         0.792683       0.832645  "
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6508 - accuracy: 0.6486 - val_loss: 0.7137 - val_accuracy: 0.5520\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6489 - accuracy: 0.6486 - val_loss: 0.7037 - val_accuracy: 0.5520\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6493 - accuracy: 0.6486 - val_loss: 0.7103 - val_accuracy: 0.5520\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6479 - accuracy: 0.6486 - val_loss: 0.7024 - val_accuracy: 0.5520\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6468 - accuracy: 0.6486 - val_loss: 0.7081 - val_accuracy: 0.5520\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6458 - accuracy: 0.6486 - val_loss: 0.7059 - val_accuracy: 0.5520\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6443 - accuracy: 0.6486 - val_loss: 0.7037 - val_accuracy: 0.5520\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6420 - accuracy: 0.6486 - val_loss: 0.7025 - val_accuracy: 0.5520\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6390 - accuracy: 0.6486 - val_loss: 0.7009 - val_accuracy: 0.5520\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6353 - accuracy: 0.6486 - val_loss: 0.6989 - val_accuracy: 0.5520\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6299 - accuracy: 0.6486 - val_loss: 0.6927 - val_accuracy: 0.5520\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6232 - accuracy: 0.6486 - val_loss: 0.6866 - val_accuracy: 0.5520\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6137 - accuracy: 0.6486 - val_loss: 0.6755 - val_accuracy: 0.5520\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6026 - accuracy: 0.6486 - val_loss: 0.6714 - val_accuracy: 0.5520\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5882 - accuracy: 0.6566 - val_loss: 0.6532 - val_accuracy: 0.5600\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5730 - accuracy: 0.6827 - val_loss: 0.6357 - val_accuracy: 0.5760\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5547 - accuracy: 0.7028 - val_loss: 0.6353 - val_accuracy: 0.5840\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5361 - accuracy: 0.7269 - val_loss: 0.6093 - val_accuracy: 0.6720\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5180 - accuracy: 0.7711 - val_loss: 0.5936 - val_accuracy: 0.6720\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4999 - accuracy: 0.7711 - val_loss: 0.5868 - val_accuracy: 0.7040\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4838 - accuracy: 0.7751 - val_loss: 0.5773 - val_accuracy: 0.7040\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.7952 - val_loss: 0.5667 - val_accuracy: 0.7200\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4573 - accuracy: 0.8273 - val_loss: 0.5563 - val_accuracy: 0.7040\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4471 - accuracy: 0.8313 - val_loss: 0.5598 - val_accuracy: 0.7040\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.8273 - val_loss: 0.5476 - val_accuracy: 0.7280\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4308 - accuracy: 0.8333 - val_loss: 0.5485 - val_accuracy: 0.7200\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4251 - accuracy: 0.8353 - val_loss: 0.5515 - val_accuracy: 0.7360\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4198 - accuracy: 0.8394 - val_loss: 0.5421 - val_accuracy: 0.7520\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.8414 - val_loss: 0.5485 - val_accuracy: 0.7440\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4126 - accuracy: 0.8373 - val_loss: 0.5501 - val_accuracy: 0.7440\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8414 - val_loss: 0.5433 - val_accuracy: 0.7600\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8434 - val_loss: 0.5440 - val_accuracy: 0.7600\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.8394 - val_loss: 0.5454 - val_accuracy: 0.7600\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4027 - accuracy: 0.8373 - val_loss: 0.5471 - val_accuracy: 0.7600\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.8373 - val_loss: 0.5457 - val_accuracy: 0.7440\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8353 - val_loss: 0.5453 - val_accuracy: 0.7520\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3974 - accuracy: 0.8414 - val_loss: 0.5463 - val_accuracy: 0.7440\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8394 - val_loss: 0.5479 - val_accuracy: 0.7520\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3950 - accuracy: 0.8454 - val_loss: 0.5468 - val_accuracy: 0.7440\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3945 - accuracy: 0.8333 - val_loss: 0.5438 - val_accuracy: 0.7440\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.8434 - val_loss: 0.5465 - val_accuracy: 0.7440\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8454 - val_loss: 0.5469 - val_accuracy: 0.7440\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8454 - val_loss: 0.5457 - val_accuracy: 0.7440\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3910 - accuracy: 0.8353 - val_loss: 0.5419 - val_accuracy: 0.7440\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8414 - val_loss: 0.5443 - val_accuracy: 0.7440\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8373 - val_loss: 0.5433 - val_accuracy: 0.7440\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8414 - val_loss: 0.5493 - val_accuracy: 0.7520\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8474 - val_loss: 0.5446 - val_accuracy: 0.7440\n",
      "Epoch 49/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8434 - val_loss: 0.5396 - val_accuracy: 0.7360\n",
      "Epoch 50/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3864 - accuracy: 0.8414 - val_loss: 0.5408 - val_accuracy: 0.7440\n",
      "Epoch 51/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3855 - accuracy: 0.8414 - val_loss: 0.5463 - val_accuracy: 0.7520\n",
      "Epoch 52/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3858 - accuracy: 0.8434 - val_loss: 0.5392 - val_accuracy: 0.7440\n",
      "Epoch 53/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.8394 - val_loss: 0.5423 - val_accuracy: 0.7520\n",
      "Epoch 54/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.8434 - val_loss: 0.5394 - val_accuracy: 0.7520\n",
      "Epoch 55/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8414 - val_loss: 0.5424 - val_accuracy: 0.7520\n",
      "Epoch 56/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.8454 - val_loss: 0.5413 - val_accuracy: 0.7520\n",
      "Epoch 57/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.8434 - val_loss: 0.5381 - val_accuracy: 0.7520\n",
      "Epoch 58/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8434 - val_loss: 0.5354 - val_accuracy: 0.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3855 - accuracy: 0.8434 - val_loss: 0.5466 - val_accuracy: 0.7520\n",
      "Epoch 60/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3804 - accuracy: 0.8454 - val_loss: 0.5350 - val_accuracy: 0.7520\n",
      "Epoch 61/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3813 - accuracy: 0.8434 - val_loss: 0.5376 - val_accuracy: 0.7520\n",
      "Epoch 62/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3805 - accuracy: 0.8414 - val_loss: 0.5408 - val_accuracy: 0.7520\n",
      "Epoch 63/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3802 - accuracy: 0.8434 - val_loss: 0.5415 - val_accuracy: 0.7520\n",
      "Epoch 64/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8414 - val_loss: 0.5418 - val_accuracy: 0.7520\n",
      "Epoch 65/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3791 - accuracy: 0.8454 - val_loss: 0.5388 - val_accuracy: 0.7520\n",
      "Epoch 66/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3795 - accuracy: 0.8414 - val_loss: 0.5405 - val_accuracy: 0.7440\n",
      "Epoch 67/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3813 - accuracy: 0.8394 - val_loss: 0.5330 - val_accuracy: 0.7520\n",
      "Epoch 68/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3789 - accuracy: 0.8474 - val_loss: 0.5402 - val_accuracy: 0.7440\n",
      "Epoch 69/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3780 - accuracy: 0.8454 - val_loss: 0.5386 - val_accuracy: 0.7440\n",
      "Epoch 70/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3819 - accuracy: 0.8454 - val_loss: 0.5345 - val_accuracy: 0.7520\n",
      "Epoch 71/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3791 - accuracy: 0.8454 - val_loss: 0.5434 - val_accuracy: 0.7440\n",
      "Epoch 72/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.8454 - val_loss: 0.5369 - val_accuracy: 0.7440\n",
      "Epoch 73/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3773 - accuracy: 0.8434 - val_loss: 0.5357 - val_accuracy: 0.7440\n",
      "Epoch 74/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3771 - accuracy: 0.8434 - val_loss: 0.5389 - val_accuracy: 0.7440\n",
      "Epoch 75/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8454 - val_loss: 0.5351 - val_accuracy: 0.7440\n",
      "Epoch 76/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3765 - accuracy: 0.8434 - val_loss: 0.5392 - val_accuracy: 0.7440\n",
      "Epoch 77/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8454 - val_loss: 0.5415 - val_accuracy: 0.7440\n",
      "Epoch 78/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3766 - accuracy: 0.8434 - val_loss: 0.5343 - val_accuracy: 0.7440\n",
      "Epoch 79/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3764 - accuracy: 0.8434 - val_loss: 0.5407 - val_accuracy: 0.7440\n",
      "Epoch 80/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3767 - accuracy: 0.8454 - val_loss: 0.5353 - val_accuracy: 0.7440\n",
      "Epoch 81/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.8434 - val_loss: 0.5399 - val_accuracy: 0.7440\n",
      "Epoch 82/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3754 - accuracy: 0.8454 - val_loss: 0.5354 - val_accuracy: 0.7440\n",
      "Epoch 83/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8414 - val_loss: 0.5371 - val_accuracy: 0.7440\n",
      "Epoch 84/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3747 - accuracy: 0.8494 - val_loss: 0.5424 - val_accuracy: 0.7440\n",
      "Epoch 85/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8514 - val_loss: 0.5415 - val_accuracy: 0.7440\n",
      "Epoch 86/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3745 - accuracy: 0.8434 - val_loss: 0.5358 - val_accuracy: 0.7440\n",
      "Epoch 87/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8474 - val_loss: 0.5398 - val_accuracy: 0.7440\n",
      "Epoch 88/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3740 - accuracy: 0.8494 - val_loss: 0.5376 - val_accuracy: 0.7440\n",
      "Epoch 89/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3739 - accuracy: 0.8474 - val_loss: 0.5392 - val_accuracy: 0.7440\n",
      "Epoch 90/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3745 - accuracy: 0.8434 - val_loss: 0.5336 - val_accuracy: 0.7440\n",
      "Epoch 91/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3748 - accuracy: 0.8494 - val_loss: 0.5409 - val_accuracy: 0.7440\n",
      "Epoch 92/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8494 - val_loss: 0.5373 - val_accuracy: 0.7520\n",
      "Epoch 93/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3736 - accuracy: 0.8494 - val_loss: 0.5388 - val_accuracy: 0.7520\n",
      "Epoch 94/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.8494 - val_loss: 0.5356 - val_accuracy: 0.7440\n",
      "Epoch 95/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3739 - accuracy: 0.8454 - val_loss: 0.5342 - val_accuracy: 0.7440\n",
      "Epoch 96/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8514 - val_loss: 0.5377 - val_accuracy: 0.7520\n",
      "Epoch 97/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8514 - val_loss: 0.5360 - val_accuracy: 0.7520\n",
      "Epoch 98/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3726 - accuracy: 0.8514 - val_loss: 0.5387 - val_accuracy: 0.7520\n",
      "Epoch 99/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8494 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 100/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8514 - val_loss: 0.5335 - val_accuracy: 0.7440\n",
      "Epoch 101/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3723 - accuracy: 0.8514 - val_loss: 0.5372 - val_accuracy: 0.7520\n",
      "Epoch 102/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3724 - accuracy: 0.8474 - val_loss: 0.5387 - val_accuracy: 0.7520\n",
      "Epoch 103/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3721 - accuracy: 0.8494 - val_loss: 0.5350 - val_accuracy: 0.7520\n",
      "Epoch 104/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8514 - val_loss: 0.5328 - val_accuracy: 0.7520\n",
      "Epoch 105/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8514 - val_loss: 0.5350 - val_accuracy: 0.7520\n",
      "Epoch 106/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3721 - accuracy: 0.8494 - val_loss: 0.5386 - val_accuracy: 0.7520\n",
      "Epoch 107/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3718 - accuracy: 0.8494 - val_loss: 0.5368 - val_accuracy: 0.7520\n",
      "Epoch 108/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8514 - val_loss: 0.5352 - val_accuracy: 0.7520\n",
      "Epoch 109/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.8514 - val_loss: 0.5371 - val_accuracy: 0.7520\n",
      "Epoch 110/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3727 - accuracy: 0.8494 - val_loss: 0.5332 - val_accuracy: 0.7520\n",
      "Epoch 111/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8474 - val_loss: 0.5398 - val_accuracy: 0.7520\n",
      "Epoch 112/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3715 - accuracy: 0.8474 - val_loss: 0.5352 - val_accuracy: 0.7520\n",
      "Epoch 113/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3734 - accuracy: 0.8514 - val_loss: 0.5331 - val_accuracy: 0.7520\n",
      "Epoch 114/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8494 - val_loss: 0.5381 - val_accuracy: 0.7520\n",
      "Epoch 115/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3715 - accuracy: 0.8494 - val_loss: 0.5424 - val_accuracy: 0.7520\n",
      "Epoch 116/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8534 - val_loss: 0.5379 - val_accuracy: 0.7520\n",
      "Epoch 117/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3709 - accuracy: 0.8494 - val_loss: 0.5350 - val_accuracy: 0.7520\n",
      "Epoch 118/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3710 - accuracy: 0.8494 - val_loss: 0.5351 - val_accuracy: 0.7520\n",
      "Epoch 119/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8494 - val_loss: 0.5403 - val_accuracy: 0.7520\n",
      "Epoch 120/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3713 - accuracy: 0.8514 - val_loss: 0.5319 - val_accuracy: 0.7520\n",
      "Epoch 121/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3710 - accuracy: 0.8494 - val_loss: 0.5362 - val_accuracy: 0.7520\n",
      "Epoch 122/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3709 - accuracy: 0.8514 - val_loss: 0.5388 - val_accuracy: 0.7520\n",
      "Epoch 123/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3704 - accuracy: 0.8494 - val_loss: 0.5333 - val_accuracy: 0.7520\n",
      "Epoch 124/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3715 - accuracy: 0.8494 - val_loss: 0.5368 - val_accuracy: 0.7520\n",
      "Epoch 125/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3704 - accuracy: 0.8494 - val_loss: 0.5371 - val_accuracy: 0.7520\n",
      "Epoch 126/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8514 - val_loss: 0.5341 - val_accuracy: 0.7520\n",
      "Epoch 127/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8494 - val_loss: 0.5364 - val_accuracy: 0.7520\n",
      "Epoch 128/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8494 - val_loss: 0.5358 - val_accuracy: 0.7520\n",
      "Epoch 129/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3707 - accuracy: 0.8494 - val_loss: 0.5349 - val_accuracy: 0.7520\n",
      "Epoch 130/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3707 - accuracy: 0.8514 - val_loss: 0.5412 - val_accuracy: 0.7520\n",
      "Epoch 131/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8534 - val_loss: 0.5372 - val_accuracy: 0.7520\n",
      "Epoch 132/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3713 - accuracy: 0.8514 - val_loss: 0.5311 - val_accuracy: 0.7520\n",
      "Epoch 133/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8514 - val_loss: 0.5380 - val_accuracy: 0.7520\n",
      "Epoch 134/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8514 - val_loss: 0.5388 - val_accuracy: 0.7520\n",
      "Epoch 135/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3712 - accuracy: 0.8494 - val_loss: 0.5315 - val_accuracy: 0.7520\n",
      "Epoch 136/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3700 - accuracy: 0.8534 - val_loss: 0.5396 - val_accuracy: 0.7520\n",
      "Epoch 137/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8534 - val_loss: 0.5358 - val_accuracy: 0.7520\n",
      "Epoch 138/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8514 - val_loss: 0.5354 - val_accuracy: 0.7520\n",
      "Epoch 139/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8534 - val_loss: 0.5359 - val_accuracy: 0.7520\n",
      "Epoch 140/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3700 - accuracy: 0.8534 - val_loss: 0.5387 - val_accuracy: 0.7520\n",
      "Epoch 141/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8514 - val_loss: 0.5342 - val_accuracy: 0.7520\n",
      "Epoch 142/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3697 - accuracy: 0.8494 - val_loss: 0.5357 - val_accuracy: 0.7520\n",
      "Epoch 143/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8514 - val_loss: 0.5369 - val_accuracy: 0.7520\n",
      "Epoch 144/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8514 - val_loss: 0.5367 - val_accuracy: 0.7520\n",
      "Epoch 145/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.8534 - val_loss: 0.5374 - val_accuracy: 0.7520\n",
      "Epoch 146/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3709 - accuracy: 0.8534 - val_loss: 0.5346 - val_accuracy: 0.7520\n",
      "Epoch 147/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8534 - val_loss: 0.5424 - val_accuracy: 0.7520\n",
      "Epoch 148/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3697 - accuracy: 0.8534 - val_loss: 0.5347 - val_accuracy: 0.7520\n",
      "Epoch 149/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8534 - val_loss: 0.5356 - val_accuracy: 0.7520\n",
      "Epoch 150/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8514 - val_loss: 0.5358 - val_accuracy: 0.7520\n",
      "Epoch 151/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8514 - val_loss: 0.5333 - val_accuracy: 0.7520\n",
      "Epoch 152/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.8514 - val_loss: 0.5418 - val_accuracy: 0.7520\n",
      "Epoch 153/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8554 - val_loss: 0.5367 - val_accuracy: 0.7520\n",
      "Epoch 154/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3693 - accuracy: 0.8534 - val_loss: 0.5351 - val_accuracy: 0.7520\n",
      "Epoch 155/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.8534 - val_loss: 0.5335 - val_accuracy: 0.7520\n",
      "Epoch 156/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.8534 - val_loss: 0.5368 - val_accuracy: 0.7520\n",
      "Epoch 157/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8534 - val_loss: 0.5331 - val_accuracy: 0.7520\n",
      "Epoch 158/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.8514 - val_loss: 0.5361 - val_accuracy: 0.7520\n",
      "Epoch 159/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8534 - val_loss: 0.5352 - val_accuracy: 0.7520\n",
      "Epoch 160/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8514 - val_loss: 0.5333 - val_accuracy: 0.7520\n",
      "Epoch 161/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8514 - val_loss: 0.5386 - val_accuracy: 0.7520\n",
      "Epoch 162/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3691 - accuracy: 0.8534 - val_loss: 0.5389 - val_accuracy: 0.7520\n",
      "Epoch 163/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8534 - val_loss: 0.5365 - val_accuracy: 0.7520\n",
      "Epoch 164/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8514 - val_loss: 0.5322 - val_accuracy: 0.7520\n",
      "Epoch 165/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8514 - val_loss: 0.5414 - val_accuracy: 0.7520\n",
      "Epoch 166/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8534 - val_loss: 0.5348 - val_accuracy: 0.7520\n",
      "Epoch 167/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3690 - accuracy: 0.8534 - val_loss: 0.5346 - val_accuracy: 0.7520\n",
      "Epoch 168/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5351 - val_accuracy: 0.7520\n",
      "Epoch 169/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3689 - accuracy: 0.8514 - val_loss: 0.5348 - val_accuracy: 0.7520\n",
      "Epoch 170/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8514 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 171/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5327 - val_accuracy: 0.7520\n",
      "Epoch 172/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8494 - val_loss: 0.5313 - val_accuracy: 0.7520\n",
      "Epoch 173/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8534 - val_loss: 0.5413 - val_accuracy: 0.7520\n",
      "Epoch 174/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3693 - accuracy: 0.8534 - val_loss: 0.5377 - val_accuracy: 0.7520\n",
      "Epoch 175/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5374 - val_accuracy: 0.7520\n",
      "Epoch 176/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8534 - val_loss: 0.5368 - val_accuracy: 0.7520\n",
      "Epoch 177/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8534 - val_loss: 0.5361 - val_accuracy: 0.7520\n",
      "Epoch 178/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8494 - val_loss: 0.5347 - val_accuracy: 0.7520\n",
      "Epoch 179/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 180/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5374 - val_accuracy: 0.7520\n",
      "Epoch 181/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5380 - val_accuracy: 0.7520\n",
      "Epoch 182/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5350 - val_accuracy: 0.7520\n",
      "Epoch 183/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.8514 - val_loss: 0.5358 - val_accuracy: 0.7520\n",
      "Epoch 184/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8534 - val_loss: 0.5393 - val_accuracy: 0.7520\n",
      "Epoch 185/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8534 - val_loss: 0.5384 - val_accuracy: 0.7520\n",
      "Epoch 186/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8534 - val_loss: 0.5379 - val_accuracy: 0.7520\n",
      "Epoch 187/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8514 - val_loss: 0.5325 - val_accuracy: 0.7520\n",
      "Epoch 188/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8534 - val_loss: 0.5407 - val_accuracy: 0.7520\n",
      "Epoch 189/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8534 - val_loss: 0.5347 - val_accuracy: 0.7520\n",
      "Epoch 190/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.8534 - val_loss: 0.5387 - val_accuracy: 0.7520\n",
      "Epoch 191/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5364 - val_accuracy: 0.7520\n",
      "Epoch 192/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8494 - val_loss: 0.5342 - val_accuracy: 0.7520\n",
      "Epoch 193/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8554 - val_loss: 0.5423 - val_accuracy: 0.7520\n",
      "Epoch 194/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8534 - val_loss: 0.5388 - val_accuracy: 0.7520\n",
      "Epoch 195/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8514 - val_loss: 0.5361 - val_accuracy: 0.7520\n",
      "Epoch 196/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5388 - val_accuracy: 0.7520\n",
      "Epoch 197/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8534 - val_loss: 0.5369 - val_accuracy: 0.7520\n",
      "Epoch 198/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8534 - val_loss: 0.5392 - val_accuracy: 0.7520\n",
      "Epoch 199/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8554 - val_loss: 0.5376 - val_accuracy: 0.7520\n",
      "Epoch 200/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8534 - val_loss: 0.5367 - val_accuracy: 0.7520\n",
      "Epoch 201/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3705 - accuracy: 0.8534 - val_loss: 0.5416 - val_accuracy: 0.7520\n",
      "Epoch 202/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8534 - val_loss: 0.5329 - val_accuracy: 0.7520\n",
      "Epoch 203/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8514 - val_loss: 0.5337 - val_accuracy: 0.7520\n",
      "Epoch 204/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8514 - val_loss: 0.5351 - val_accuracy: 0.7520\n",
      "Epoch 205/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5388 - val_accuracy: 0.7520\n",
      "Epoch 206/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8554 - val_loss: 0.5356 - val_accuracy: 0.7520\n",
      "Epoch 207/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5375 - val_accuracy: 0.7520\n",
      "Epoch 208/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5383 - val_accuracy: 0.7520\n",
      "Epoch 209/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.8554 - val_loss: 0.5426 - val_accuracy: 0.7520\n",
      "Epoch 210/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8534 - val_loss: 0.5370 - val_accuracy: 0.7520\n",
      "Epoch 211/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8514 - val_loss: 0.5381 - val_accuracy: 0.7520\n",
      "Epoch 212/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8534 - val_loss: 0.5352 - val_accuracy: 0.7520\n",
      "Epoch 213/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8514 - val_loss: 0.5349 - val_accuracy: 0.7520\n",
      "Epoch 214/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8534 - val_loss: 0.5361 - val_accuracy: 0.7520\n",
      "Epoch 215/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8554 - val_loss: 0.5420 - val_accuracy: 0.7520\n",
      "Epoch 216/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8534 - val_loss: 0.5393 - val_accuracy: 0.7520\n",
      "Epoch 217/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8494 - val_loss: 0.5351 - val_accuracy: 0.7520\n",
      "Epoch 218/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5422 - val_accuracy: 0.7520\n",
      "Epoch 219/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8514 - val_loss: 0.5422 - val_accuracy: 0.7520\n",
      "Epoch 220/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8534 - val_loss: 0.5335 - val_accuracy: 0.7520\n",
      "Epoch 221/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.8534 - val_loss: 0.5391 - val_accuracy: 0.7520\n",
      "Epoch 222/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8534 - val_loss: 0.5362 - val_accuracy: 0.7520\n",
      "Epoch 223/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3679 - accuracy: 0.8534 - val_loss: 0.5361 - val_accuracy: 0.7520\n",
      "Epoch 224/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8534 - val_loss: 0.5428 - val_accuracy: 0.7520\n",
      "Epoch 225/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8514 - val_loss: 0.5336 - val_accuracy: 0.7520\n",
      "Epoch 226/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.8534 - val_loss: 0.5366 - val_accuracy: 0.7520\n",
      "Epoch 227/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3691 - accuracy: 0.8514 - val_loss: 0.5311 - val_accuracy: 0.7520\n",
      "Epoch 228/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3702 - accuracy: 0.8554 - val_loss: 0.5423 - val_accuracy: 0.7520\n",
      "Epoch 229/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8514 - val_loss: 0.5329 - val_accuracy: 0.7520\n",
      "Epoch 230/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8534 - val_loss: 0.5373 - val_accuracy: 0.7520\n",
      "Epoch 231/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8534 - val_loss: 0.5390 - val_accuracy: 0.7520\n",
      "Epoch 232/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5378 - val_accuracy: 0.7520\n",
      "Epoch 233/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8534 - val_loss: 0.5398 - val_accuracy: 0.7520\n",
      "Epoch 234/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8514 - val_loss: 0.5406 - val_accuracy: 0.7520\n",
      "Epoch 235/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8534 - val_loss: 0.5371 - val_accuracy: 0.7520\n",
      "Epoch 236/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8534 - val_loss: 0.5359 - val_accuracy: 0.7520\n",
      "Epoch 237/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8514 - val_loss: 0.5347 - val_accuracy: 0.7520\n",
      "Epoch 238/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8514 - val_loss: 0.5368 - val_accuracy: 0.7520\n",
      "Epoch 239/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8534 - val_loss: 0.5372 - val_accuracy: 0.7520\n",
      "Epoch 240/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8534 - val_loss: 0.5361 - val_accuracy: 0.7520\n",
      "Epoch 241/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3677 - accuracy: 0.8514 - val_loss: 0.5405 - val_accuracy: 0.7520\n",
      "Epoch 242/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5395 - val_accuracy: 0.7520\n",
      "Epoch 243/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8534 - val_loss: 0.5361 - val_accuracy: 0.7520\n",
      "Epoch 244/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 245/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 246/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8514 - val_loss: 0.5335 - val_accuracy: 0.7520\n",
      "Epoch 247/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8494 - val_loss: 0.5416 - val_accuracy: 0.7520\n",
      "Epoch 248/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8514 - val_loss: 0.5385 - val_accuracy: 0.7520\n",
      "Epoch 249/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3678 - accuracy: 0.8514 - val_loss: 0.5393 - val_accuracy: 0.7520\n",
      "Epoch 250/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.8514 - val_loss: 0.5404 - val_accuracy: 0.7520\n",
      "Epoch 251/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8514 - val_loss: 0.5385 - val_accuracy: 0.7520\n",
      "Epoch 252/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8514 - val_loss: 0.5338 - val_accuracy: 0.7520\n",
      "Epoch 253/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8534 - val_loss: 0.5371 - val_accuracy: 0.7520\n",
      "Epoch 254/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8534 - val_loss: 0.5377 - val_accuracy: 0.7520\n",
      "Epoch 255/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3683 - accuracy: 0.8534 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 256/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8534 - val_loss: 0.5400 - val_accuracy: 0.7520\n",
      "Epoch 257/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3679 - accuracy: 0.8534 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 258/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8514 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 259/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3679 - accuracy: 0.8514 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 260/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3675 - accuracy: 0.8534 - val_loss: 0.5386 - val_accuracy: 0.7520\n",
      "Epoch 261/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3679 - accuracy: 0.8514 - val_loss: 0.5388 - val_accuracy: 0.7520\n",
      "Epoch 262/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3679 - accuracy: 0.8534 - val_loss: 0.5356 - val_accuracy: 0.7520\n",
      "Epoch 263/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8534 - val_loss: 0.5366 - val_accuracy: 0.7520\n",
      "Epoch 264/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8514 - val_loss: 0.5407 - val_accuracy: 0.7520\n",
      "Epoch 265/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3677 - accuracy: 0.8534 - val_loss: 0.5400 - val_accuracy: 0.7520\n",
      "Epoch 266/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8514 - val_loss: 0.5350 - val_accuracy: 0.7520\n",
      "Epoch 267/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8534 - val_loss: 0.5353 - val_accuracy: 0.7520\n",
      "Epoch 268/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8494 - val_loss: 0.5417 - val_accuracy: 0.7520\n",
      "Epoch 269/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8534 - val_loss: 0.5399 - val_accuracy: 0.7520\n",
      "Epoch 270/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8534 - val_loss: 0.5350 - val_accuracy: 0.7520\n",
      "Epoch 271/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8534 - val_loss: 0.5375 - val_accuracy: 0.7520\n",
      "Epoch 272/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3678 - accuracy: 0.8534 - val_loss: 0.5414 - val_accuracy: 0.7520\n",
      "Epoch 273/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3672 - accuracy: 0.8534 - val_loss: 0.5351 - val_accuracy: 0.7520\n",
      "Epoch 274/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8534 - val_loss: 0.5359 - val_accuracy: 0.7520\n",
      "Epoch 275/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8514 - val_loss: 0.5403 - val_accuracy: 0.7520\n",
      "Epoch 276/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8534 - val_loss: 0.5377 - val_accuracy: 0.7520\n",
      "Epoch 277/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8534 - val_loss: 0.5356 - val_accuracy: 0.7520\n",
      "Epoch 278/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8514 - val_loss: 0.5395 - val_accuracy: 0.7520\n",
      "Epoch 279/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8514 - val_loss: 0.5366 - val_accuracy: 0.7520\n",
      "Epoch 280/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3679 - accuracy: 0.8514 - val_loss: 0.5388 - val_accuracy: 0.7520\n",
      "Epoch 281/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3672 - accuracy: 0.8514 - val_loss: 0.5383 - val_accuracy: 0.7520\n",
      "Epoch 282/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8534 - val_loss: 0.5370 - val_accuracy: 0.7520\n",
      "Epoch 283/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3672 - accuracy: 0.8534 - val_loss: 0.5364 - val_accuracy: 0.7520\n",
      "Epoch 284/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8534 - val_loss: 0.5366 - val_accuracy: 0.7520\n",
      "Epoch 285/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8534 - val_loss: 0.5359 - val_accuracy: 0.7520\n",
      "Epoch 286/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8514 - val_loss: 0.5401 - val_accuracy: 0.7520\n",
      "Epoch 287/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5369 - val_accuracy: 0.7520\n",
      "Epoch 288/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8574 - val_loss: 0.5404 - val_accuracy: 0.7520\n",
      "Epoch 289/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3671 - accuracy: 0.8534 - val_loss: 0.5370 - val_accuracy: 0.7520\n",
      "Epoch 290/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3672 - accuracy: 0.8534 - val_loss: 0.5370 - val_accuracy: 0.7520\n",
      "Epoch 291/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5358 - val_accuracy: 0.7520\n",
      "Epoch 292/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3671 - accuracy: 0.8514 - val_loss: 0.5383 - val_accuracy: 0.7520\n",
      "Epoch 293/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8534 - val_loss: 0.5393 - val_accuracy: 0.7520\n",
      "Epoch 294/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5336 - val_accuracy: 0.7520\n",
      "Epoch 295/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8534 - val_loss: 0.5388 - val_accuracy: 0.7520\n",
      "Epoch 296/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5363 - val_accuracy: 0.7520\n",
      "Epoch 297/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8534 - val_loss: 0.5372 - val_accuracy: 0.7520\n",
      "Epoch 298/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5476 - val_accuracy: 0.7520\n",
      "Epoch 299/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3679 - accuracy: 0.8554 - val_loss: 0.5358 - val_accuracy: 0.7520\n",
      "Epoch 300/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8534 - val_loss: 0.5366 - val_accuracy: 0.7520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scl',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Age', 'Fare']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(use_cat_names=True))]),\n",
       "                                                  ['Sex', 'Embarked', 'Pclass',\n",
       "                                                   'Parch', 'SibSp'])])),\n",
       "                ('clf',\n",
       "                 <keras.wrappers.scikit_learn.KerasClassifier object at 0x000001DABEDBF6A0>)])"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features_ is 24 and input n_features is 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-546-4835e7074919>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mlog_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_holdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     models_quality = models_quality.append({'Name':models_string[i], \n\u001b[0;32m     21\u001b[0m                                                 \u001b[1;34m'accuracy_score'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_holdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             raise ValueError(\"Number of features of the model must \"\n\u001b[0m\u001b[0;32m    801\u001b[0m                              \u001b[1;34mf\"match the input. Model n_features_ is {self._n_features} and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m                              f\"input n_features is {n_features}\")\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features_ is 24 and input n_features is 7"
     ]
    }
   ],
   "source": [
    "#обучаем модели, которые  требуют категориального преобразования \n",
    "# CatBoostRegressor\n",
    "\n",
    "\n",
    "# lightgbm\n",
    "\n",
    "gbm = lgb.LGBMRegressor()\n",
    "gbm.fit(X_train_prep, y_train, eval_set=[(X_holdout_prep, y_holdout)],verbose=False)\n",
    "\n",
    "\n",
    "# выводим результат\n",
    "\n",
    "models_names = [gbm,model_neural]\n",
    "models_string = [ 'gbm', 'model_neural']\n",
    "\n",
    "\n",
    "for i in range(len(models_names)):\n",
    "    model = models_names[i]\n",
    "    log_pred = np.round(model.predict(X_holdout), 0)\n",
    "    models_quality = models_quality.append({'Name':models_string[i], \n",
    "                                                'accuracy_score':accuracy_score(y_holdout, log_pred),\n",
    "                                                'recall_score':recall_score(y_holdout, log_pred),\n",
    "                                               'f1_score':f1_score(y_holdout, log_pred),\n",
    "                                                'precision_score':precision_score(y_holdout, log_pred),\n",
    "                                               'roc_auc_score':roc_auc_score(y_holdout, log_pred)}, ignore_index=True)\n",
    "#     except ValueError:\n",
    "#         prediction_binary = list(map(round, log_pred))\n",
    "#         models_quality = models_quality.append({'Name':models_string[i],\n",
    "#                                                 'accuracy_score':accuracy_score(y_holdout, prediction_binary),\n",
    "#                                                 'recall_score':recall_score(y_holdout, prediction_binary),\n",
    "#                                                 'f1_score':f1_score(y_holdout, prediction_binary),\n",
    "#                                                 'precision_score':precision_score(y_holdout, prediction_binary),\n",
    "#                                                'roc_auc_score':roc_auc_score(y_holdout, prediction_binary)}, ignore_index=True)\n",
    "models_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.6539 - accuracy: 0.6486 - val_loss: 0.7081 - val_accuracy: 0.5520\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6488 - accuracy: 0.6486 - val_loss: 0.7112 - val_accuracy: 0.5520\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6479 - accuracy: 0.6486 - val_loss: 0.7075 - val_accuracy: 0.5520\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.6486 - val_loss: 0.7059 - val_accuracy: 0.5520\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6468 - accuracy: 0.6486 - val_loss: 0.7049 - val_accuracy: 0.5520\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6463 - accuracy: 0.6486 - val_loss: 0.7076 - val_accuracy: 0.5520\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.6486 - val_loss: 0.7039 - val_accuracy: 0.5520\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6433 - accuracy: 0.6486 - val_loss: 0.7025 - val_accuracy: 0.5520\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.6419 - accuracy: 0.6486 - val_loss: 0.7050 - val_accuracy: 0.5520\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6380 - accuracy: 0.6486 - val_loss: 0.6980 - val_accuracy: 0.5520\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6345 - accuracy: 0.6486 - val_loss: 0.6970 - val_accuracy: 0.5520\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6293 - accuracy: 0.6486 - val_loss: 0.6896 - val_accuracy: 0.5520\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6218 - accuracy: 0.6486 - val_loss: 0.6870 - val_accuracy: 0.5520\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.6111 - accuracy: 0.6486 - val_loss: 0.6723 - val_accuracy: 0.5520\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5987 - accuracy: 0.6486 - val_loss: 0.6608 - val_accuracy: 0.5520\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5823 - accuracy: 0.6586 - val_loss: 0.6468 - val_accuracy: 0.5600\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.6908 - val_loss: 0.6317 - val_accuracy: 0.5760\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.7189 - val_loss: 0.6168 - val_accuracy: 0.6480\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5208 - accuracy: 0.7651 - val_loss: 0.5945 - val_accuracy: 0.6800\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5003 - accuracy: 0.7711 - val_loss: 0.5854 - val_accuracy: 0.7040\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.7851 - val_loss: 0.5702 - val_accuracy: 0.7200\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.8173 - val_loss: 0.5618 - val_accuracy: 0.7200\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4527 - accuracy: 0.8273 - val_loss: 0.5525 - val_accuracy: 0.7120\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.8273 - val_loss: 0.5531 - val_accuracy: 0.7120\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4332 - accuracy: 0.8293 - val_loss: 0.5527 - val_accuracy: 0.7200\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8273 - val_loss: 0.5467 - val_accuracy: 0.7520\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4221 - accuracy: 0.8333 - val_loss: 0.5444 - val_accuracy: 0.7600\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4172 - accuracy: 0.8333 - val_loss: 0.5489 - val_accuracy: 0.7440\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4133 - accuracy: 0.8353 - val_loss: 0.5512 - val_accuracy: 0.7440\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4118 - accuracy: 0.8414 - val_loss: 0.5414 - val_accuracy: 0.7600\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8373 - val_loss: 0.5466 - val_accuracy: 0.7440\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8353 - val_loss: 0.5475 - val_accuracy: 0.7440\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8353 - val_loss: 0.5455 - val_accuracy: 0.7520\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.8373 - val_loss: 0.5466 - val_accuracy: 0.7520\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8353 - val_loss: 0.5401 - val_accuracy: 0.7520\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8373 - val_loss: 0.5512 - val_accuracy: 0.7440\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8414 - val_loss: 0.5475 - val_accuracy: 0.7520\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8394 - val_loss: 0.5474 - val_accuracy: 0.7440\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.8394 - val_loss: 0.5462 - val_accuracy: 0.7440\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3930 - accuracy: 0.8394 - val_loss: 0.5442 - val_accuracy: 0.7440\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3923 - accuracy: 0.8373 - val_loss: 0.5424 - val_accuracy: 0.7440\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8313 - val_loss: 0.5431 - val_accuracy: 0.7440\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8394 - val_loss: 0.5414 - val_accuracy: 0.7440\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8414 - val_loss: 0.5491 - val_accuracy: 0.7600\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8454 - val_loss: 0.5399 - val_accuracy: 0.7360\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3883 - accuracy: 0.8434 - val_loss: 0.5447 - val_accuracy: 0.7520\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8454 - val_loss: 0.5480 - val_accuracy: 0.7520\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3864 - accuracy: 0.8474 - val_loss: 0.5382 - val_accuracy: 0.7440\n",
      "Epoch 49/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3870 - accuracy: 0.8414 - val_loss: 0.5409 - val_accuracy: 0.7440\n",
      "Epoch 50/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3861 - accuracy: 0.8454 - val_loss: 0.5416 - val_accuracy: 0.7520\n",
      "Epoch 51/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3851 - accuracy: 0.8454 - val_loss: 0.5375 - val_accuracy: 0.7440\n",
      "Epoch 52/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3848 - accuracy: 0.8454 - val_loss: 0.5384 - val_accuracy: 0.7520\n",
      "Epoch 53/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.8434 - val_loss: 0.5449 - val_accuracy: 0.7520\n",
      "Epoch 54/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8414 - val_loss: 0.5404 - val_accuracy: 0.7520\n",
      "Epoch 55/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3835 - accuracy: 0.8454 - val_loss: 0.5409 - val_accuracy: 0.7520\n",
      "Epoch 56/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8434 - val_loss: 0.5345 - val_accuracy: 0.7520\n",
      "Epoch 57/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8434 - val_loss: 0.5400 - val_accuracy: 0.7520\n",
      "Epoch 58/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8434 - val_loss: 0.5384 - val_accuracy: 0.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8474 - val_loss: 0.5406 - val_accuracy: 0.7520\n",
      "Epoch 60/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8454 - val_loss: 0.5372 - val_accuracy: 0.7520\n",
      "Epoch 61/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8434 - val_loss: 0.5408 - val_accuracy: 0.7440\n",
      "Epoch 62/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8454 - val_loss: 0.5378 - val_accuracy: 0.7520\n",
      "Epoch 63/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8434 - val_loss: 0.5390 - val_accuracy: 0.7440\n",
      "Epoch 64/300\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3273 - accuracy: 0.78 - 0s 1ms/step - loss: 0.3796 - accuracy: 0.8414 - val_loss: 0.5409 - val_accuracy: 0.7440\n",
      "Epoch 65/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3790 - accuracy: 0.8474 - val_loss: 0.5366 - val_accuracy: 0.7440\n",
      "Epoch 66/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3794 - accuracy: 0.8434 - val_loss: 0.5420 - val_accuracy: 0.7440\n",
      "Epoch 67/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3795 - accuracy: 0.8434 - val_loss: 0.5362 - val_accuracy: 0.7440\n",
      "Epoch 68/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3780 - accuracy: 0.8474 - val_loss: 0.5392 - val_accuracy: 0.7440\n",
      "Epoch 69/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.8454 - val_loss: 0.5431 - val_accuracy: 0.7440\n",
      "Epoch 70/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3779 - accuracy: 0.8454 - val_loss: 0.5355 - val_accuracy: 0.7440\n",
      "Epoch 71/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3775 - accuracy: 0.8434 - val_loss: 0.5346 - val_accuracy: 0.7440\n",
      "Epoch 72/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3787 - accuracy: 0.8454 - val_loss: 0.5389 - val_accuracy: 0.7440\n",
      "Epoch 73/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3770 - accuracy: 0.8474 - val_loss: 0.5378 - val_accuracy: 0.7440\n",
      "Epoch 74/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3772 - accuracy: 0.8434 - val_loss: 0.5358 - val_accuracy: 0.7440\n",
      "Epoch 75/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3765 - accuracy: 0.8394 - val_loss: 0.5368 - val_accuracy: 0.7440\n",
      "Epoch 76/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3763 - accuracy: 0.8434 - val_loss: 0.5366 - val_accuracy: 0.7440\n",
      "Epoch 77/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3756 - accuracy: 0.8414 - val_loss: 0.5390 - val_accuracy: 0.7440\n",
      "Epoch 78/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3756 - accuracy: 0.8454 - val_loss: 0.5405 - val_accuracy: 0.7440\n",
      "Epoch 79/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3765 - accuracy: 0.8434 - val_loss: 0.5345 - val_accuracy: 0.7440\n",
      "Epoch 80/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3752 - accuracy: 0.8434 - val_loss: 0.5382 - val_accuracy: 0.7440\n",
      "Epoch 81/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.8414 - val_loss: 0.5355 - val_accuracy: 0.7440\n",
      "Epoch 82/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3760 - accuracy: 0.8494 - val_loss: 0.5398 - val_accuracy: 0.7440\n",
      "Epoch 83/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3756 - accuracy: 0.8454 - val_loss: 0.5339 - val_accuracy: 0.7440\n",
      "Epoch 84/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8434 - val_loss: 0.5382 - val_accuracy: 0.7440\n",
      "Epoch 85/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3747 - accuracy: 0.8434 - val_loss: 0.5347 - val_accuracy: 0.7440\n",
      "Epoch 86/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3751 - accuracy: 0.8474 - val_loss: 0.5405 - val_accuracy: 0.7440\n",
      "Epoch 87/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3743 - accuracy: 0.8414 - val_loss: 0.5360 - val_accuracy: 0.7440\n",
      "Epoch 88/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8474 - val_loss: 0.5371 - val_accuracy: 0.7440\n",
      "Epoch 89/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3745 - accuracy: 0.8474 - val_loss: 0.5424 - val_accuracy: 0.7520\n",
      "Epoch 90/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3751 - accuracy: 0.8514 - val_loss: 0.5345 - val_accuracy: 0.7440\n",
      "Epoch 91/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3738 - accuracy: 0.8474 - val_loss: 0.5390 - val_accuracy: 0.7520\n",
      "Epoch 92/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.8494 - val_loss: 0.5380 - val_accuracy: 0.7520\n",
      "Epoch 93/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8494 - val_loss: 0.5377 - val_accuracy: 0.7520\n",
      "Epoch 94/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8494 - val_loss: 0.5369 - val_accuracy: 0.7520\n",
      "Epoch 95/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3729 - accuracy: 0.8494 - val_loss: 0.5350 - val_accuracy: 0.7440\n",
      "Epoch 96/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8494 - val_loss: 0.5371 - val_accuracy: 0.7520\n",
      "Epoch 97/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3723 - accuracy: 0.8474 - val_loss: 0.5398 - val_accuracy: 0.7520\n",
      "Epoch 98/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8494 - val_loss: 0.5336 - val_accuracy: 0.7440\n",
      "Epoch 99/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3722 - accuracy: 0.8494 - val_loss: 0.5407 - val_accuracy: 0.7520\n",
      "Epoch 100/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8494 - val_loss: 0.5407 - val_accuracy: 0.7520\n",
      "Epoch 101/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3738 - accuracy: 0.8494 - val_loss: 0.5328 - val_accuracy: 0.7520\n",
      "Epoch 102/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3722 - accuracy: 0.8514 - val_loss: 0.5347 - val_accuracy: 0.7520\n",
      "Epoch 103/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8494 - val_loss: 0.5334 - val_accuracy: 0.7520\n",
      "Epoch 104/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8534 - val_loss: 0.5428 - val_accuracy: 0.7520\n",
      "Epoch 105/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3716 - accuracy: 0.8534 - val_loss: 0.5353 - val_accuracy: 0.7520\n",
      "Epoch 106/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3715 - accuracy: 0.8494 - val_loss: 0.5344 - val_accuracy: 0.7520\n",
      "Epoch 107/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3720 - accuracy: 0.8494 - val_loss: 0.5357 - val_accuracy: 0.7520\n",
      "Epoch 108/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3713 - accuracy: 0.8494 - val_loss: 0.5325 - val_accuracy: 0.7520\n",
      "Epoch 109/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3721 - accuracy: 0.8494 - val_loss: 0.5353 - val_accuracy: 0.7520\n",
      "Epoch 110/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3714 - accuracy: 0.8494 - val_loss: 0.5367 - val_accuracy: 0.7520\n",
      "Epoch 111/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3719 - accuracy: 0.8494 - val_loss: 0.5321 - val_accuracy: 0.7520\n",
      "Epoch 112/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3713 - accuracy: 0.8494 - val_loss: 0.5369 - val_accuracy: 0.7520\n",
      "Epoch 113/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8494 - val_loss: 0.5344 - val_accuracy: 0.7520\n",
      "Epoch 114/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8514 - val_loss: 0.5370 - val_accuracy: 0.7520\n",
      "Epoch 115/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3710 - accuracy: 0.8494 - val_loss: 0.5358 - val_accuracy: 0.7520\n",
      "Epoch 116/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8494 - val_loss: 0.5310 - val_accuracy: 0.7520\n",
      "Epoch 117/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3714 - accuracy: 0.8494 - val_loss: 0.5343 - val_accuracy: 0.7520\n",
      "Epoch 118/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3712 - accuracy: 0.8494 - val_loss: 0.5342 - val_accuracy: 0.7520\n",
      "Epoch 119/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3709 - accuracy: 0.8494 - val_loss: 0.5385 - val_accuracy: 0.7520\n",
      "Epoch 120/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8514 - val_loss: 0.5377 - val_accuracy: 0.7520\n",
      "Epoch 121/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3710 - accuracy: 0.8494 - val_loss: 0.5362 - val_accuracy: 0.7520\n",
      "Epoch 122/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3707 - accuracy: 0.8494 - val_loss: 0.5331 - val_accuracy: 0.7520\n",
      "Epoch 123/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3715 - accuracy: 0.8494 - val_loss: 0.5321 - val_accuracy: 0.7520\n",
      "Epoch 124/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3716 - accuracy: 0.8514 - val_loss: 0.5398 - val_accuracy: 0.7520\n",
      "Epoch 125/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3718 - accuracy: 0.8514 - val_loss: 0.5313 - val_accuracy: 0.7520\n",
      "Epoch 126/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8514 - val_loss: 0.5357 - val_accuracy: 0.7520\n",
      "Epoch 127/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3707 - accuracy: 0.8554 - val_loss: 0.5386 - val_accuracy: 0.7520\n",
      "Epoch 128/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3704 - accuracy: 0.8534 - val_loss: 0.5332 - val_accuracy: 0.7520\n",
      "Epoch 129/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8494 - val_loss: 0.5343 - val_accuracy: 0.7520\n",
      "Epoch 130/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3702 - accuracy: 0.8494 - val_loss: 0.5334 - val_accuracy: 0.7520\n",
      "Epoch 131/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3705 - accuracy: 0.8534 - val_loss: 0.5377 - val_accuracy: 0.7520\n",
      "Epoch 132/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8534 - val_loss: 0.5358 - val_accuracy: 0.7520\n",
      "Epoch 133/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8494 - val_loss: 0.5351 - val_accuracy: 0.7520\n",
      "Epoch 134/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3714 - accuracy: 0.8494 - val_loss: 0.5348 - val_accuracy: 0.7520\n",
      "Epoch 135/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8494 - val_loss: 0.5372 - val_accuracy: 0.7520\n",
      "Epoch 136/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3707 - accuracy: 0.8514 - val_loss: 0.5341 - val_accuracy: 0.7520\n",
      "Epoch 137/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8494 - val_loss: 0.5349 - val_accuracy: 0.7520\n",
      "Epoch 138/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8514 - val_loss: 0.5394 - val_accuracy: 0.7520\n",
      "Epoch 139/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8494 - val_loss: 0.5348 - val_accuracy: 0.7520\n",
      "Epoch 140/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8494 - val_loss: 0.5383 - val_accuracy: 0.7520\n",
      "Epoch 141/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8494 - val_loss: 0.5325 - val_accuracy: 0.7520\n",
      "Epoch 142/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.8514 - val_loss: 0.5362 - val_accuracy: 0.7520\n",
      "Epoch 143/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3701 - accuracy: 0.8534 - val_loss: 0.5395 - val_accuracy: 0.7520\n",
      "Epoch 144/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3712 - accuracy: 0.8514 - val_loss: 0.5339 - val_accuracy: 0.7520\n",
      "Epoch 145/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8514 - val_loss: 0.5381 - val_accuracy: 0.7520\n",
      "Epoch 146/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8534 - val_loss: 0.5329 - val_accuracy: 0.7520\n",
      "Epoch 147/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.8494 - val_loss: 0.5347 - val_accuracy: 0.7520\n",
      "Epoch 148/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3697 - accuracy: 0.8514 - val_loss: 0.5370 - val_accuracy: 0.7520\n",
      "Epoch 149/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8534 - val_loss: 0.5359 - val_accuracy: 0.7520\n",
      "Epoch 150/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3696 - accuracy: 0.8534 - val_loss: 0.5371 - val_accuracy: 0.7520\n",
      "Epoch 151/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8534 - val_loss: 0.5381 - val_accuracy: 0.7520\n",
      "Epoch 152/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8534 - val_loss: 0.5376 - val_accuracy: 0.7520\n",
      "Epoch 153/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3701 - accuracy: 0.8514 - val_loss: 0.5322 - val_accuracy: 0.7520\n",
      "Epoch 154/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8514 - val_loss: 0.5384 - val_accuracy: 0.7520\n",
      "Epoch 155/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3693 - accuracy: 0.8534 - val_loss: 0.5357 - val_accuracy: 0.7520\n",
      "Epoch 156/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8514 - val_loss: 0.5368 - val_accuracy: 0.7520\n",
      "Epoch 157/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8534 - val_loss: 0.5390 - val_accuracy: 0.7520\n",
      "Epoch 158/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8534 - val_loss: 0.5361 - val_accuracy: 0.7520\n",
      "Epoch 159/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8514 - val_loss: 0.5359 - val_accuracy: 0.7520\n",
      "Epoch 160/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8514 - val_loss: 0.5367 - val_accuracy: 0.7520\n",
      "Epoch 161/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8534 - val_loss: 0.5381 - val_accuracy: 0.7520\n",
      "Epoch 162/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8514 - val_loss: 0.5336 - val_accuracy: 0.7520\n",
      "Epoch 163/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8534 - val_loss: 0.5379 - val_accuracy: 0.7520\n",
      "Epoch 164/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3700 - accuracy: 0.8514 - val_loss: 0.5352 - val_accuracy: 0.7520\n",
      "Epoch 165/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3700 - accuracy: 0.8494 - val_loss: 0.5381 - val_accuracy: 0.7520\n",
      "Epoch 166/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8514 - val_loss: 0.5427 - val_accuracy: 0.7520\n",
      "Epoch 167/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5345 - val_accuracy: 0.7520\n",
      "Epoch 168/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8514 - val_loss: 0.5353 - val_accuracy: 0.7520\n",
      "Epoch 169/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8494 - val_loss: 0.5324 - val_accuracy: 0.7520\n",
      "Epoch 170/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3691 - accuracy: 0.8514 - val_loss: 0.5372 - val_accuracy: 0.7520\n",
      "Epoch 171/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8554 - val_loss: 0.5398 - val_accuracy: 0.7520\n",
      "Epoch 172/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8514 - val_loss: 0.5367 - val_accuracy: 0.7520\n",
      "Epoch 173/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8534 - val_loss: 0.5380 - val_accuracy: 0.7520\n",
      "Epoch 174/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8514 - val_loss: 0.5384 - val_accuracy: 0.7520\n",
      "Epoch 175/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8514 - val_loss: 0.5387 - val_accuracy: 0.7520\n",
      "Epoch 176/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8534 - val_loss: 0.5376 - val_accuracy: 0.7520\n",
      "Epoch 177/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8494 - val_loss: 0.5336 - val_accuracy: 0.7520\n",
      "Epoch 178/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8514 - val_loss: 0.5386 - val_accuracy: 0.7520\n",
      "Epoch 179/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5393 - val_accuracy: 0.7520\n",
      "Epoch 180/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8514 - val_loss: 0.5360 - val_accuracy: 0.7520\n",
      "Epoch 181/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.8534 - val_loss: 0.5386 - val_accuracy: 0.7520\n",
      "Epoch 182/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8534 - val_loss: 0.5369 - val_accuracy: 0.7520\n",
      "Epoch 183/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8534 - val_loss: 0.5364 - val_accuracy: 0.7520\n",
      "Epoch 184/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5357 - val_accuracy: 0.7520\n",
      "Epoch 185/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3693 - accuracy: 0.8534 - val_loss: 0.5361 - val_accuracy: 0.7520\n",
      "Epoch 186/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3702 - accuracy: 0.8534 - val_loss: 0.5420 - val_accuracy: 0.7520\n",
      "Epoch 187/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.8554 - val_loss: 0.5345 - val_accuracy: 0.7520\n",
      "Epoch 188/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3691 - accuracy: 0.8514 - val_loss: 0.5356 - val_accuracy: 0.7520\n",
      "Epoch 189/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3691 - accuracy: 0.8494 - val_loss: 0.5334 - val_accuracy: 0.7520\n",
      "Epoch 190/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8534 - val_loss: 0.5395 - val_accuracy: 0.7520\n",
      "Epoch 191/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 192/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5393 - val_accuracy: 0.7520\n",
      "Epoch 193/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8554 - val_loss: 0.5387 - val_accuracy: 0.7520\n",
      "Epoch 194/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3702 - accuracy: 0.8494 - val_loss: 0.5329 - val_accuracy: 0.7520\n",
      "Epoch 195/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3683 - accuracy: 0.8514 - val_loss: 0.5380 - val_accuracy: 0.7520\n",
      "Epoch 196/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3683 - accuracy: 0.8514 - val_loss: 0.5392 - val_accuracy: 0.7520\n",
      "Epoch 197/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8514 - val_loss: 0.5362 - val_accuracy: 0.7520\n",
      "Epoch 198/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8514 - val_loss: 0.5341 - val_accuracy: 0.7520\n",
      "Epoch 199/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3684 - accuracy: 0.8514 - val_loss: 0.5374 - val_accuracy: 0.7520\n",
      "Epoch 200/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8534 - val_loss: 0.5381 - val_accuracy: 0.7520\n",
      "Epoch 201/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8534 - val_loss: 0.5365 - val_accuracy: 0.7520\n",
      "Epoch 202/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5397 - val_accuracy: 0.7520\n",
      "Epoch 203/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8514 - val_loss: 0.5350 - val_accuracy: 0.7520\n",
      "Epoch 204/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3683 - accuracy: 0.8534 - val_loss: 0.5407 - val_accuracy: 0.7520\n",
      "Epoch 205/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8534 - val_loss: 0.5369 - val_accuracy: 0.7520\n",
      "Epoch 206/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8534 - val_loss: 0.5395 - val_accuracy: 0.7520\n",
      "Epoch 207/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5383 - val_accuracy: 0.7520\n",
      "Epoch 208/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3683 - accuracy: 0.8514 - val_loss: 0.5385 - val_accuracy: 0.7520\n",
      "Epoch 209/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8514 - val_loss: 0.5339 - val_accuracy: 0.7520\n",
      "Epoch 210/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8534 - val_loss: 0.5379 - val_accuracy: 0.7520\n",
      "Epoch 211/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8534 - val_loss: 0.5427 - val_accuracy: 0.7520\n",
      "Epoch 212/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8534 - val_loss: 0.5398 - val_accuracy: 0.7520\n",
      "Epoch 213/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5399 - val_accuracy: 0.7520\n",
      "Epoch 214/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.8534 - val_loss: 0.5393 - val_accuracy: 0.7520\n",
      "Epoch 215/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5371 - val_accuracy: 0.7520\n",
      "Epoch 216/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8534 - val_loss: 0.5361 - val_accuracy: 0.7520\n",
      "Epoch 217/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3681 - accuracy: 0.8534 - val_loss: 0.5375 - val_accuracy: 0.7520\n",
      "Epoch 218/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3686 - accuracy: 0.8534 - val_loss: 0.5381 - val_accuracy: 0.7520\n",
      "Epoch 219/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3702 - accuracy: 0.8534 - val_loss: 0.5415 - val_accuracy: 0.7520\n",
      "Epoch 220/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8514 - val_loss: 0.5298 - val_accuracy: 0.7520\n",
      "Epoch 221/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8514 - val_loss: 0.5353 - val_accuracy: 0.7520\n",
      "Epoch 222/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5364 - val_accuracy: 0.7520\n",
      "Epoch 223/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.8514 - val_loss: 0.5368 - val_accuracy: 0.7520\n",
      "Epoch 224/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8514 - val_loss: 0.5369 - val_accuracy: 0.7520\n",
      "Epoch 225/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8514 - val_loss: 0.5395 - val_accuracy: 0.7520\n",
      "Epoch 226/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8534 - val_loss: 0.5369 - val_accuracy: 0.7520\n",
      "Epoch 227/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8514 - val_loss: 0.5351 - val_accuracy: 0.7520\n",
      "Epoch 228/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8514 - val_loss: 0.5399 - val_accuracy: 0.7520\n",
      "Epoch 229/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8514 - val_loss: 0.5333 - val_accuracy: 0.7520\n",
      "Epoch 230/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5374 - val_accuracy: 0.7520\n",
      "Epoch 231/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5387 - val_accuracy: 0.7520\n",
      "Epoch 232/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8514 - val_loss: 0.5380 - val_accuracy: 0.7520\n",
      "Epoch 233/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8514 - val_loss: 0.5330 - val_accuracy: 0.7520\n",
      "Epoch 234/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8534 - val_loss: 0.5390 - val_accuracy: 0.7520\n",
      "Epoch 235/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8534 - val_loss: 0.5368 - val_accuracy: 0.7520\n",
      "Epoch 236/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8514 - val_loss: 0.5376 - val_accuracy: 0.7520\n",
      "Epoch 237/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8514 - val_loss: 0.5365 - val_accuracy: 0.7520\n",
      "Epoch 238/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3693 - accuracy: 0.8494 - val_loss: 0.5356 - val_accuracy: 0.7520\n",
      "Epoch 239/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3675 - accuracy: 0.8534 - val_loss: 0.5401 - val_accuracy: 0.7520\n",
      "Epoch 240/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.8514 - val_loss: 0.5414 - val_accuracy: 0.7520\n",
      "Epoch 241/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.8514 - val_loss: 0.5385 - val_accuracy: 0.7520\n",
      "Epoch 242/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8534 - val_loss: 0.5347 - val_accuracy: 0.7520\n",
      "Epoch 243/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3685 - accuracy: 0.8534 - val_loss: 0.5382 - val_accuracy: 0.7520\n",
      "Epoch 244/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8514 - val_loss: 0.5346 - val_accuracy: 0.7520\n",
      "Epoch 245/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3678 - accuracy: 0.8534 - val_loss: 0.5359 - val_accuracy: 0.7520\n",
      "Epoch 246/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8514 - val_loss: 0.5395 - val_accuracy: 0.7520\n",
      "Epoch 247/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.8514 - val_loss: 0.5383 - val_accuracy: 0.7520\n",
      "Epoch 248/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8514 - val_loss: 0.5386 - val_accuracy: 0.7520\n",
      "Epoch 249/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8534 - val_loss: 0.5366 - val_accuracy: 0.7520\n",
      "Epoch 250/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8534 - val_loss: 0.5337 - val_accuracy: 0.7520\n",
      "Epoch 251/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3685 - accuracy: 0.8514 - val_loss: 0.5398 - val_accuracy: 0.7520\n",
      "Epoch 252/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8534 - val_loss: 0.5403 - val_accuracy: 0.7520\n",
      "Epoch 253/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8534 - val_loss: 0.5341 - val_accuracy: 0.7520\n",
      "Epoch 254/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8534 - val_loss: 0.5356 - val_accuracy: 0.7520\n",
      "Epoch 255/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8534 - val_loss: 0.5373 - val_accuracy: 0.7520\n",
      "Epoch 256/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8534 - val_loss: 0.5372 - val_accuracy: 0.7520\n",
      "Epoch 257/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3678 - accuracy: 0.8534 - val_loss: 0.5355 - val_accuracy: 0.7520\n",
      "Epoch 258/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8534 - val_loss: 0.5384 - val_accuracy: 0.7520\n",
      "Epoch 259/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8514 - val_loss: 0.5373 - val_accuracy: 0.7520\n",
      "Epoch 260/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8514 - val_loss: 0.5365 - val_accuracy: 0.7520\n",
      "Epoch 261/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8534 - val_loss: 0.5357 - val_accuracy: 0.7520\n",
      "Epoch 262/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3678 - accuracy: 0.8534 - val_loss: 0.5347 - val_accuracy: 0.7520\n",
      "Epoch 263/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3679 - accuracy: 0.8514 - val_loss: 0.5367 - val_accuracy: 0.7520\n",
      "Epoch 264/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3691 - accuracy: 0.8534 - val_loss: 0.5389 - val_accuracy: 0.7520\n",
      "Epoch 265/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.8534 - val_loss: 0.5407 - val_accuracy: 0.7520\n",
      "Epoch 266/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8534 - val_loss: 0.5347 - val_accuracy: 0.7520\n",
      "Epoch 267/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8534 - val_loss: 0.5376 - val_accuracy: 0.7520\n",
      "Epoch 268/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3683 - accuracy: 0.8534 - val_loss: 0.5375 - val_accuracy: 0.7520\n",
      "Epoch 269/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8534 - val_loss: 0.5394 - val_accuracy: 0.7520\n",
      "Epoch 270/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8534 - val_loss: 0.5389 - val_accuracy: 0.7520\n",
      "Epoch 271/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8534 - val_loss: 0.5427 - val_accuracy: 0.7520\n",
      "Epoch 272/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8514 - val_loss: 0.5389 - val_accuracy: 0.7520\n",
      "Epoch 273/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8554 - val_loss: 0.5355 - val_accuracy: 0.7520\n",
      "Epoch 274/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5367 - val_accuracy: 0.7520\n",
      "Epoch 275/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8514 - val_loss: 0.5400 - val_accuracy: 0.7520\n",
      "Epoch 276/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8534 - val_loss: 0.5344 - val_accuracy: 0.7520\n",
      "Epoch 277/300\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.4565 - accuracy: 0.84 - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8554 - val_loss: 0.5392 - val_accuracy: 0.7520\n",
      "Epoch 278/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3677 - accuracy: 0.8534 - val_loss: 0.5428 - val_accuracy: 0.7520\n",
      "Epoch 279/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3679 - accuracy: 0.8514 - val_loss: 0.5356 - val_accuracy: 0.7520\n",
      "Epoch 280/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8534 - val_loss: 0.5367 - val_accuracy: 0.7520\n",
      "Epoch 281/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8534 - val_loss: 0.5405 - val_accuracy: 0.7520\n",
      "Epoch 282/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3677 - accuracy: 0.8534 - val_loss: 0.5421 - val_accuracy: 0.7520\n",
      "Epoch 283/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8534 - val_loss: 0.5334 - val_accuracy: 0.7520\n",
      "Epoch 284/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8534 - val_loss: 0.5393 - val_accuracy: 0.7520\n",
      "Epoch 285/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8534 - val_loss: 0.5367 - val_accuracy: 0.7520\n",
      "Epoch 286/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8534 - val_loss: 0.5374 - val_accuracy: 0.7520\n",
      "Epoch 287/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3676 - accuracy: 0.8554 - val_loss: 0.5403 - val_accuracy: 0.7520\n",
      "Epoch 288/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3677 - accuracy: 0.8514 - val_loss: 0.5411 - val_accuracy: 0.7520\n",
      "Epoch 289/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3671 - accuracy: 0.8554 - val_loss: 0.5372 - val_accuracy: 0.7520\n",
      "Epoch 290/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8514 - val_loss: 0.5345 - val_accuracy: 0.7520\n",
      "Epoch 291/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3672 - accuracy: 0.8534 - val_loss: 0.5419 - val_accuracy: 0.7520\n",
      "Epoch 292/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8534 - val_loss: 0.5385 - val_accuracy: 0.7520\n",
      "Epoch 293/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8554 - val_loss: 0.5425 - val_accuracy: 0.7520\n",
      "Epoch 294/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8514 - val_loss: 0.5386 - val_accuracy: 0.7520\n",
      "Epoch 295/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8534 - val_loss: 0.5391 - val_accuracy: 0.7520\n",
      "Epoch 296/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3675 - accuracy: 0.8534 - val_loss: 0.5403 - val_accuracy: 0.7520\n",
      "Epoch 297/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3677 - accuracy: 0.8534 - val_loss: 0.5412 - val_accuracy: 0.7520\n",
      "Epoch 298/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3671 - accuracy: 0.8534 - val_loss: 0.5370 - val_accuracy: 0.7520\n",
      "Epoch 299/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8534 - val_loss: 0.5358 - val_accuracy: 0.7520\n",
      "Epoch 300/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8554 - val_loss: 0.5401 - val_accuracy: 0.7520\n",
      "Wall time: 8.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Попробуйте переключить verbose = 1 и будет чудо!\n",
    "model_neural = model.fit(X_train_prep, to_categorical(y_train), validation_split=0.2, epochs=300, verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
